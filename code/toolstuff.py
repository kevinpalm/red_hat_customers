import pandas as pd
import numpy as np


def simple_load():

    """
    :return:
        train : pandas.DataFrame
            The kaggle training dataset joined with the kaggle people dataset
        test : pandas.DataFrame
            The kaggle testing dataset joined with the kaggle people dataset
    """

    # Read in the data
    people = pd.read_csv("../data/people.csv")
    train = pd.read_csv("../data/act_train.csv")
    test = pd.read_csv("../data/act_test.csv")

    # Mark unique groups of people
    grp = people.copy()
    grp["group_size"] = 1
    grp = grp.groupby("group_1", as_index=False)["group_size"].count()
    grp["unique_group"] = grp["group_size"] = 1
    people = people.merge(grp, on="group_1", how="left")

    # Merge people to the other data sets
    train = train.merge(people, on="people_id", suffixes=("_act", ""))
    test = test.merge(people, on="people_id", suffixes=("_act", ""))

    # Set index to activity id
    train = train.set_index("activity_id")
    test = test.set_index("activity_id")

    # Correct some data types
    for field in ["date_act", "date"]:
        train[field] = pd.to_datetime(train[field])
        test[field] = pd.to_datetime(test[field])

    return train, test


def group_decision(train, test):

    """
    :param train: pandas.DataFrame
        The kaggle training dataset joined with the kaggle people dataset
    :param test:  pandas.DataFrame
        The kaggle testing dataset joined with the kaggle people dataset
    :return:
        test["outcome"] : pandas.Series
            Outcome predictions generated by using the leak/exploit model, with a matching index to test
    """

    # Exploit the leak revealed by Loiso and team to try and directly infer any labels that can be inferred
    # https://www.kaggle.com/c/predicting-red-hat-business-value/forums/t/22807/0-987-kernel-now-available-seems-like-leakage

    # Make a lookup dataframe, and copy those in first since we can be sure of them
    lookup = train.groupby(["group_1", "date_act"], as_index=False)["outcome"].mean()
    test = pd.merge(test.reset_index(), lookup, how="left", on=["group_1", "date_act"]).set_index("activity_id")

    # Create some date filling columns that we'll use after we append
    train["date_act_fillfw"] = train["date_act"]
    train["date_act_fillbw"] = train["date_act"]

    # Create some group filling columns for later use
    train["group_fillfw"] = train["group_1"]
    train["group_fillbw"] = train["group_1"]

    # Put the two data sets together and sort
    df = train.append(test)
    df = df.sort_values(by=["group_1", "date_act"])

    # Fill the dates
    df["date_act_fillfw"] = df["date_act_fillfw"].fillna(method="ffill")
    df["date_act_fillbw"] = df["date_act_fillbw"].fillna(method="bfill")

    # Fill labels
    df["outcome_fillfw"] = df["outcome"].fillna(method="ffill")
    df["outcome_fillbw"] = df["outcome"].fillna(method="bfill")

    # Fill the groups
    df["group_fillfw"] = df["group_fillfw"].fillna(method="ffill")
    df["group_fillbw"] = df["group_fillbw"].fillna(method="bfill")

    # Create int booleans for whether the fillers are from the same date
    df["fw_same_date"] = (df["date_act_fillfw"] == df["date_act"]).astype(int)
    df["bw_same_date"] = (df["date_act_fillbw"] == df["date_act"]).astype(int)

    # Create int booleans for whether the fillers are in the same group
    df["fw_same_group"] = (df["group_fillfw"] == df["group_1"]).astype(int)
    df["bw_same_group"] = (df["group_fillbw"] == df["group_1"]).astype(int)

    # Use the filled labels only if the labels were from the same group, unless we're at the end of the group
    df["interfill"] = (df["outcome_fillfw"] *
                       df["fw_same_group"] +
                       df["outcome_fillbw"] *
                       df["bw_same_group"]) / (df["fw_same_group"] +
                                               df["bw_same_group"])

    # If the labels are at the end of the group, cushion by 0.5
    df["needs cushion"] = (df["fw_same_group"] * df["bw_same_group"] - 1).abs()
    df["cushion"] = df["needs cushion"] * df["interfill"] * -0.1 + df["needs cushion"] * 0.05
    df["interfill"] = df["interfill"] + df["cushion"]

    # Fill everything
    df["outcome"] = df["outcome"].fillna(df["interfill"])

    # Return outcomes to the original index
    test["outcome"] = df["outcome"]

    return test["outcome"]


def leak_resplit(train, test):

    # Append the two together for creating features and sort by date and group
    df = train[["group_1", "date_act", "outcome"]].append(test[["group_1", "date_act"]])

    # Groupby to calculate the mean label for the same dates in the same group
    lookup = train.groupby(["group_1", "date_act"], as_index=False)["outcome"].mean()
    lookup.columns = ["group_1", "date_act", "mean_outcome"]

    # Merge those means into the original sets
    df = pd.merge(df.reset_index(), lookup, how="left", on=["group_1", "date_act"]).set_index("activity_id")

    # Fill nans in outcome with mean outcome for those testing samples with definite leaks
    test["outcome"] = df["mean_outcome"]

    # Resplit
    train = train.append(test[test["outcome"].notnull()])
    test = test[test["outcome"].isnull()]

    return train, test


def extract_leak_features(train, test):

    """
    :param train: pandas.DataFrame
        The kaggle training dataset joined with the kaggle people dataset
    :param test:  pandas.DataFrame
        The kaggle testing dataset joined with the kaggle people dataset
    :return:
        train_x : pandas.DataFrame
            Training features pertaining to the leak, split to include extra leaked rows
        train_y : pandas.Series
            Training labels pertaining to the leak, split to include extra leaked rows, index matches train_x
        test_x : pandas.DataFrame
            Testing features pertaing to the leak, split to exclude extra leaked rows
    """

    # Drop the extra columns
    train = train[["group_1", "date_act", "outcome"]]
    test = test[["group_1", "date_act"]]

    # Append the two together for creating features and sort by date and group
    df = train.append(test).sort_values(by=["group_1", "date_act"])

    # Copy train onto the merged index, then shift a copy one step each way for neighbor features, fill accordingly
    indextrain = pd.DataFrame(index=df.index).join(train)
    df = df.join(indextrain.shift(-1), rsuffix="_bfill").join(indextrain.shift(1), rsuffix="_ffill")
    df[["group_1_bfill", "date_act_bfill", "outcome_bfill"]] =\
        df[["group_1_bfill", "date_act_bfill", "outcome_bfill"]].fillna(method="bfill")
    df[["group_1_ffill", "date_act_ffill", "outcome_ffill"]] =\
        df[["group_1_ffill", "date_act_ffill", "outcome_ffill"]].fillna(method="ffill")

    # Set anything that was filled between groups as nan
    df.loc[(df["group_1"] != df["group_1_ffill"]), ["group_1_ffill", "date_act_ffill", "outcome_ffill"]] = np.nan
    df.loc[(df["group_1"] != df["group_1_bfill"]), ["group_1_bfill", "date_act_bfill", "outcome_bfill"]] = np.nan

    # We want to know the range and average density for each group
    lookup = pd.DataFrame()
    lookup["min_date"] = df.groupby("group_1")["date_act"].min()
    lookup["max_date"] = df.groupby("group_1")["date_act"].max()
    lookup["range"] = (lookup["max_date"] - lookup["min_date"])/np.timedelta64(1, 'D')
    lookup["data_count"] = df.groupby("group_1")["date_act"].count()
    lookup["density"] = lookup["data_count"]/lookup["range"]
    lookup = lookup[["density", "range"]].reset_index()

    # Merge in density
    df = pd.merge(df.reset_index(), lookup, how="left", on=["group_1"]).set_index("activity_id")

    # Groupby to calculate the mean label for the same dates in the same group
    lookup = train.groupby(["group_1", "date_act"], as_index=False)["outcome"].mean()
    lookup.columns = ["group_1", "date_act", "mean_outcome"]

    # Merge those means into the original sets
    df = pd.merge(df.reset_index(), lookup, how="left", on=["group_1", "date_act"]).set_index("activity_id")

    # Get distance to each side as a proportion of the group density... weird math units but seems to work
    df["right_distance_density"] = (df["date_act"]-df["date_act_ffill"])/np.timedelta64(1, 'D')/df["density"]
    df["left_distance_density"] = (df["date_act_bfill"]-df["date_act"])/np.timedelta64(1, 'D')/df["density"]
    df["right_distance_range"] = (df["date_act"]-df["date_act_ffill"])/np.timedelta64(1, 'D')/df["range"]
    df["left_distance_range"] = (df["date_act_bfill"]-df["date_act"])/np.timedelta64(1, 'D')/df["range"]

    # Get values to each side
    df["left_outcome"] = df["outcome_bfill"]
    df["right_outcome"] = df["outcome_ffill"]

    # Replace infinities from the groups with 0 distance with nans
    df = df.replace(np.inf, np.nan)

    # Fill nans in outcome with mean outcome for those testing samples with definite leaks
    df["outcome"] = df["outcome"].fillna(df["mean_outcome"])

    # Try and retrieve the mapping if its already been created
    try:
        # Read in the map file
        map = pd.read_csv("../output/group_cluster_map.csv").set_index("activity_id")

        # Join the map into the df
        df = df.join(map)

    except:
        pass

    # Resplit the data now that we have more training data
    train = df[pd.notnull(df["outcome"])]
    test = df[pd.isnull(df["outcome"])]

    # Select out features for final output
    train_x = train[["left_distance_density", "right_distance_density", "left_outcome", "right_outcome",
                     "left_distance_range", "right_distance_range"]]
    train_y = train["outcome"]
    test_x = test[["left_distance_density", "right_distance_density", "left_outcome", "right_outcome",
                   "left_distance_range", "right_distance_range"]]

    return train_x, train_y, test_x


def prep_features(train, test):

    """
    :param train: pandas.DataFrame
        The kaggle training dataset joined with the kaggle people dataset
    :param test:  pandas.DataFrame
        The kaggle testing dataset joined with the kaggle people dataset
    :return:
        train_comps: pd.DataFrame
            Training principle components of features indexed to match train, or the extra outcomes if given
        test_comps: pd.Dataframe
            Testing principle components of features indexed to match test, and exclude the extra outcomes if given
    """

    # Initialize two empty data frames with matching indexes for storing features
    train_feats = pd.DataFrame(index=train.index)
    test_feats = pd.DataFrame(index=test.index)

    # Seasonal features for activity date, we only need <1 year types because that's all the training data includes
    train_feats = train_feats.join(pd.get_dummies("act_day_" + train["date_act"].dt.day.astype(str)))
    train_feats = train_feats.join(pd.get_dummies("act_month_" + train["date_act"].dt.month.astype(str)))
    train_feats = train_feats.join(pd.get_dummies("act_weekday_" + train["date_act"].dt.weekday.astype(str)))
    test_feats = test_feats.join(pd.get_dummies("act_day_" + test["date_act"].dt.day.astype(str)))
    test_feats = test_feats.join(pd.get_dummies("act_month_" + test["date_act"].dt.month.astype(str)))
    test_feats = test_feats.join(pd.get_dummies("act_weekday_" + test["date_act"].dt.weekday.astype(str)))

    # Mark weekends
    train_feats["act_weekend"] = (train["date_act"].dt.weekday < 5).astype(int)
    test_feats["act_weekend"] = (test["date_act"].dt.weekday < 5).astype(int)

    # Seasonal features for people date, let's only do month and year because it seems to be a longer term thing
    train_feats = train_feats.join(pd.get_dummies("people_month_" + train["date"].dt.month.astype(str)))
    train_feats = train_feats.join(pd.get_dummies("people_year_" + train["date"].dt.year.astype(str)))
    test_feats = test_feats.join(pd.get_dummies("people_month_" + test["date"].dt.month.astype(str)))
    test_feats = test_feats.join(pd.get_dummies("people_year_" + test["date"].dt.year.astype(str)))

    # Drop those date columns now that they're not needed
    train = train.drop(["date", "date_act"], axis=1)
    test = test.drop(["date", "date_act"], axis=1)

    # Also remove group_1 since it's already used, and people_id by association. Also, outcome isn't a training feature
    train = train.drop(["group_1", "outcome", "people_id"], axis=1)
    test = test.drop(["group_1", "people_id"], axis=1)
    try:
        test = test.drop("outcome", axis=1)
    except:
        pass

    # Char_38 can go in as is since it's an ordinal feature, so long as we scale it to be between 0 and 1
    train_feats["char_38"] = train["char_38"]/100
    train = train.drop("char_38", axis=1)
    test_feats["char_38"] = test["char_38"]/100
    test = test.drop("char_38", axis=1)

    # Group size can also go in the same, so long as we scale it to be between 0 and 1
    scaler = train["group_size"].max()
    train_feats["group_size"] = train["group_size"]/scaler
    train = train.drop("group_size", axis=1)
    test_feats["group_size"] = test["group_size"]/scaler
    test = test.drop("group_size", axis=1)

    # Copy over the ready made booleans, one-hot the categorical with a reasonable number of features, drop as we go
    for column in train.columns.values:
        if train[column].dtype == bool:
            train_feats[column] = train[column].astype(int)
            train = train.drop(column, axis=1)
        elif len(set(train[column].tolist())) < 100:
            train_feats = train_feats.join(pd.get_dummies(column + "_" + train[column].astype(str)))
            train = train.drop(column, axis=1)

    for column in test.columns.values:
        if test[column].dtype == bool:
            test_feats[column] = test[column].astype(int)
            test = test.drop(column, axis=1)
        elif len(set(test[column].tolist())) < 100:
            test_feats = test_feats.join(pd.get_dummies(column + "_" + test[column].astype(str)))
            test = test.drop(column, axis=1)

    # Cross check for any columns that don't exist in both sides of the split
    for column in train_feats.columns.values:
        if column not in test_feats.columns.values:
            train_feats = train_feats.drop(column, axis=1)
    for column in test_feats.columns.values:
        if column not in train_feats.columns.values:
            test_feats = test_feats.drop(column, axis=1)

    return train_feats, test_feats


def subsplit_genre(train, test, traingenre, testgenre, outcomes):

    # Attach any extra know outcomes to test
    test["outcome"] = outcomes

    # Append together train and test, then break amongst the subsections and drop the genre column
    df = train.append(test)
    traingenre = traingenre.join(df).drop("genre", axis=1)
    testgenre = testgenre.join(df).drop("genre", axis=1)

    return traingenre, testgenre


def unpack_values(tup):
    df = pd.DataFrame()
    try:
        tup = np.array(tup)
        tup = pd.DataFrame(tup)
        for col in tup.columns.values:
            df[col] = tup[col]
    except:
        count = 0
        for i in tup:
            count+=1
            i = np.array(i)
            i = pd.DataFrame(i)
            for x in i:
                df[str(x)+"_"+str(count)] = i[x]
    return df


def cluster_groups_delta(group_1, trainfeats, testfeats):

    # Append the features into one dataset and ensure that group_1 is present
    df = trainfeats.append(testfeats)
    df["group_1"] = group_1

    # Load the group clusters
    map = pd.read_csv("../output/group_cluster_map.csv").set_index("activity_id")
    df["group_cluster"] = map["group_cluster"]


    # Condense to rows of groups, merge back onto the df
    grp = df.groupby(["group_1", "group_cluster"], as_index=False).mean()
    df = df.reset_index().merge(grp, on=["group_1", "group_cluster"], how="left",
                                suffixes=["", "_mean"]).set_index("activity_id")

    # Calculate delta mean
    for column in df.columns.values:
        if "_mean" not in column:
            try:
                df[column] = df[column + "_mean"] - df[column]
                trainfeats[column] = df[column]
                testfeats[column] = df[column]
            except:
                pass

    return trainfeats, testfeats


def same_extremes(df, train, test):
    """Sink or elevate to the most extreme prediction for the same groups on the same dates"""

    # Merge together the data
    lookup = df.join(train.append(test)[["group_1", "date_act"]])

    # Caluculate the easy ones
    grp = pd.DataFrame()
    grp["count"] = lookup.groupby(["group_1", "date_act"])["outcome"].count()
    grp["min"] = lookup.groupby(["group_1", "date_act"])["outcome"].min()
    grp["max"] = lookup.groupby(["group_1", "date_act"])["outcome"].max()
    grp = grp[grp["count"] > 1]
    grp["value"] = None
    grp.loc[(grp["max"] < 0.5), "value"] = grp["min"]
    grp.loc[(grp["min"] > 0.5), "value"] = grp["max"]

    # Do the remaining ones by loop
    for index, row in grp[grp["value"].isnull()].iterrows():
        if 0.5-row["min"] > row["max"]-0.5:
            grp.loc[index, "value"] = row["min"]
        else:
            grp.loc[index, "value"] = row["max"]

    # Merge to lookup for indexing and filling
    lookup = lookup.reset_index().merge(grp[["value"]].reset_index(), how="left", on=["group_1", "date_act"]
                                        ).set_index("activity_id")
    lookup["value"] = lookup["value"].fillna(lookup["outcome"])

    df["outcome"] = lookup["outcome"]

    return df
